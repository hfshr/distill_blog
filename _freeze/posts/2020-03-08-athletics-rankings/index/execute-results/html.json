{
  "hash": "cb4a46b309ff8b13eaeb6d41747c58b0",
  "result": {
    "markdown": "---\ntitle: \"Athletics rankings\"\ndescription: |\n    Using R to gather data from an athletics rankings website.\nimage: images/rr.png    \ndate: 03/08/2020\ncategories: \n  - athletics\nformat: html\nexecute: \n  cache: true\n---\n\nAs a keen athlete I spent many hours on athletics rankings website \nhttps://www.thepowerof10.info.\nFor athletics nerds, the site is fantastic and provides all the information you need to keep up to date with the latest results.\nHowever, I wondered if it might be possible to explore some alternative ways of presenting the results...\n\n# Getting the data\n\nInitially, I had to figure out a way to first gather the data from the power of 10 website. \nAs you can see in the screenshot below, the rankings are already in a table format, so thankfully it wasn't too difficult to figure something out.\n\n::: {.cell .column-page hash='index_cache/html/unnamed-chunk-2_8ec6ee9ad7b8982db1c2da6d4697e846'}\n\n```{.r .cell-code}\nknitr::include_graphics(\"images/powerof10homepage.PNG\")\n```\n\n::: {.cell-output-display}\n![Figure from https://www.thepowerof10.info](images/powerof10homepage.PNG){width=489 class=external}\n:::\n:::\n\n\nThis was my first time using the [`rvest`](http://rvest.tidyverse.org/) package, so I'm sure this code can be improved somewhat... It does, however, seem to do the trick.\nTo start, I identified a url from one of the ranking pages that would serve as a starting point to access the site.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_f18fb25e794a08fc2e75fc2c146ea4db'}\n\n```{.r .cell-code}\nurl <- \"https://www.thepowerof10.info/rankings/rankinglist.aspx?event=100&agegroup=ALL&sex=M&year=2012\"\n```\n:::\n\nTo access other pages it was just a case of supplying new values to the arguments in the url. \nFor example to get the 200m rankings I could just swap the \"event=100\" for \"event=200\" in the string. To do this, I wrote a simple function that would generate a vector of strings with the desired events, years and for male or female athletes.\n\n::: {.cell hash='index_cache/html/url_0e7bdc2050c1b0650dc196b8e4e70efb'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(zoo)\nlibrary(knitr)\nlibrary(lubridate)\n\nurlmaker <- function(years, events, gender) {\n  url <- \"https://www.thepowerof10.info/rankings/rankinglist.aspx?event=100&agegroup=ALL&sex=M&year=2012\"\n  baseevent <- as.character(str_extract_all(url, \"[0-9]+\")[[1]][[2]])\n  baseyear <- as.character(str_extract_all(url, \"[0-9]+\")[[1]][[3]])\n  newyear <- rep(years, times = length(events))\n  list_of_years <- str_replace(url, baseyear, newyear)\n  newevent <- rep(events, each = length(years))\n  yearandevent <- str_replace(list_of_years, baseevent, newevent)\n\n  sex <- str_c(\"sex=\", gender, sep = \"\")\n  yearandevent <- str_replace(yearandevent, \"sex=M\", sex)\n  return(yearandevent)\n}\n```\n:::\n\nI then created a string of events and years I wanted to scrape. \nIn this instance I only wanted male athletes, so also set gender to \"M\".\nThe result was a vector of urls as character strings that I could use to access the necessary pages.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_4ea3a84ec1376fb0d7c0f850bc47f33d'}\n\n```{.r .cell-code}\nevents <- c(\"100\", \"200\", \"400\", \"800\", \"1500\", \"3000\", \"5000\", \"10000\")\nyears <- as.character(seq(from = 2006, to = 2018, by = 1))\ngender <- \"M\"\n\n# create list of urls to use for scraping\nurls <- urlmaker(years, events, gender)\n\nprint(head(urls, 3))\n```\n\n::: {.cell-output-stdout}\n```\n[1] \"https://www.thepowerof10.info/rankings/rankinglist.aspx?event=100&agegroup=ALL&sex=M&year=2006\"\n[2] \"https://www.thepowerof10.info/rankings/rankinglist.aspx?event=100&agegroup=ALL&sex=M&year=2007\"\n[3] \"https://www.thepowerof10.info/rankings/rankinglist.aspx?event=100&agegroup=ALL&sex=M&year=2008\"\n```\n:::\n:::\n\nNext, I had to figure out a way to actually get the data. \nInspecting the html code on the page helped to identify the labels for the tables that I needed.\n\n::: {.cell .column-page hash='index_cache/html/unnamed-chunk-8_bb3183b57f0e7416b95743854de3b8a9'}\n\n```{.r .cell-code}\nknitr::include_graphics(\"images/po10_window.png\")\n```\n\n::: {.cell-output-display}\n![Figure from https://www.thepowerof10.info](images/po10_window.png){width=857 class=external}\n:::\n:::\n\n\nOnce I knew how the different elements on the page were identified, it was as easy as copying and pasting the table id into `html_nodes()` and converting to a table with `html_table()`.\nAfter that it was just the usual wrangling you'd expect with a messy data frame.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_6262a802690e5d78e45771e85e49b192'}\n\n```{.r .cell-code}\nlibrary(rvest)\n\nreadtable <- function(url) {\n  main <- read_html(url)\n  rankings <- main %>%\n    html_nodes(xpath = '//*[@id=\"cphBody_lblCachedRankingList\"]/table') %>%\n    html_table() %>%\n    data.frame() %>%\n    select(1:13) %>%\n    set_names(c(\"rank\", \n                \"perf\", \n                \"windy\", \n                \"windspeed\", \n                \"PB\", \n                \"newpb\", \n                \"name\", \n                \"agegroup\", \n                \"month_year\", \n                \"coach\", \n                \"club\", \n                \"venue\", \n                \"date\")) %>%\n    mutate(\n      year = as.numeric(str_extract_all(url, \"[0-9]+\")[[1]][[3]]),\n      event = as.numeric(str_extract_all(url, \"[0-9]+\")[[1]][[2]])\n    ) %>%\n    mutate_at(vars(name), list(~ replace(., . == \"\", NA))) %>%\n    mutate(name = na.locf(name))\n}\n```\n:::\n\n\nNow I simply had to pass the url to the function, and we'll have the table from the webpage stored as a dataframe.\n\n::: {.cell .column-page hash='index_cache/html/unnamed-chunk-12_637680915b72973a91388bfbc5777a99'}\n\n```{.r .cell-code}\nranks <- readtable(url)\n\nranks %>%\n  slice(4:6) %>%\n  select(rank, perf, name, year, club, venue) %>% \n  kable()\n```\n\n::: {.cell-output-display}\n|rank |perf  |name           | year|club                 |venue          |\n|:----|:-----|:--------------|----:|:--------------------|:--------------|\n|1    |10.02 |Dwain Chambers | 2012|Belgrave             |Olympic Park   |\n|2    |10.05 |Adam Gemili    | 2012|Blackheath & Bromley |Barcelona, ESP |\n|3    |10.13 |James Dasaolu  | 2012|Croydon              |Olympic Park   |\n:::\n:::\n\nIf I wanted to scrape multiple urls at once I would use `map_df`. \nHere I can pass my vector of urls and `map_df` will perform the scraping function `readtable` on each element in the vector and append the result together (this can take a while to complete so I'll just use the first 3 urls from our vector of 104 urls).\n\n::: {.cell hash='index_cache/html/unnamed-chunk-14_e7d65af944acd263d59a8942f37239ed'}\n\n```{.r .cell-code}\nurls_short <- urls[1:3]\n\nmale_rankings <- urls_short %>%\n  map_df(readtable)\n```\n:::\n\nNow we can view the top ranked athlete for each year.\n\n::: {.cell .column-page hash='index_cache/html/unnamed-chunk-16_69e4c1b73e5d83721c29fefdf980de8e'}\n\n```{.r .cell-code}\nmale_rankings %>%\n  filter(rank == \"1\") %>%\n  select(year, rank, perf, name, club, venue) %>% \n  kable()\n```\n\n::: {.cell-output-display}\n| year|rank |perf  |name            |club     |venue         |\n|----:|:----|:-----|:---------------|:--------|:-------------|\n| 2006|1    |10.07 |Dwain Chambers  |Belgrave |Gateshead     |\n| 2007|1    |10.06 |Marlon Devonish |Coventry |Lausanne, SUI |\n| 2008|1    |10.00 |Dwain Chambers  |Belgrave |Birmingham    |\n:::\n:::\n\n\n## Individual athletes\n\nThe PO10 also provides detailed performance history for individual athletes. \nI wanted to be able to have access to this data as well, however I wanted to avoid downloading every individual athletes data to disk as I imagine that may have taken a while...\n\nInstead the solution I came up with was a function get each individual athlete's unique identifier number. \nWith this number I could get each athletes individual rankings when required using an \"on the fly\" scrape, as a single athletes page is not very much data at all.\n\nThe screenshot below shows how the code that I needed to extract\n\n\n::: {.cell .column-page hash='index_cache/html/unnamed-chunk-18_2583550b625e50dc4edc73be3c2af01a'}\n\n```{.r .cell-code}\nknitr::include_graphics(\"images/po10id.png\")\n```\n\n::: {.cell-output-display}\n![Figure from https://www.thepowerof10.info](images/po10id.png){width=858 class=external}\n:::\n:::\n\n\nThe function below collects the athltes name, unique url, year and event.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-20_6f5478b15d35f2ffe344edc7dd34c82a'}\n\n```{.r .cell-code}\nathleteurl <- function(url) {\n  main <- read_html(url)\n  athleteinfo <- tibble(\n    name = html_text(html_nodes(main, \"td:nth-child(7) a\"), \"href\"),\n    athleteurl = html_attr(html_nodes(main, \"td:nth-child(7) a\"), \"href\"),\n    year = as.numeric(str_extract_all(url, \"[0-9]+\")[[1]][[3]]),\n    event = as.numeric(str_extract_all(url, \"[0-9]+\")[[1]][[2]])\n  ) %>%\n    filter(name != \"\")\n}\n\nids <- athleteurl(url)\n\nhead(ids)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 × 4\n  name                   athleteurl                               year event\n  <chr>                  <chr>                                   <dbl> <dbl>\n1 Dwain Chambers         /athletes/profile.aspx?athleteid=31816   2012   100\n2 Adam Gemili            /athletes/profile.aspx?athleteid=208735  2012   100\n3 James Dasaolu          /athletes/profile.aspx?athleteid=22721   2012   100\n4 Harry Aikines-Aryeetey /athletes/profile.aspx?athleteid=19988   2012   100\n5 Mark Lewis-Francis     /athletes/profile.aspx?athleteid=21139   2012   100\n6 James Alaka            /athletes/profile.aspx?athleteid=22255   2012   100\n```\n:::\n:::\n\n\nIn another function I join these two tables together so I had one data frame that had all the results and rankings as well as each athletes individual id that I could use to get their individual data.\nI also appended the full address to each individual athletes id. \n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-22_1c0a124d44c6cefc3f8bd5826c06fd6d'}\n\n```{.r .cell-code}\nfinaljoin <- function(ranks, ids, gender) {\n  yeartimes <- ranks %>%\n    group_by(year, event) %>%\n    filter(str_detect(rank, \"[:alpha:]\")) %>%\n    select(year, rank) %>%\n    filter(str_detect(rank, \"^UK\")) %>%\n    separate(rank, c(\"topn\", \"timing\"), \": \") %>%\n    spread(key = topn, value = timing) %>%\n    ungroup()\n  cleanrankings <- ranks %>%\n    filter(!str_detect(rank, \"[:alpha:]\")) %>%\n    inner_join(., ids, by = c(\"name\", \"year\", \"event\")) %>%\n    mutate(athleteurl = paste(\"https://www.thepowerof10.info\", athleteurl, sep = \"\"))\n\n  cleanrankings <- left_join(cleanrankings, yeartimes, by = c(\"event\", \"year\")) %>%\n    janitor::clean_names() %>%\n    mutate(gender = gender)\n}\n```\n:::\n\nThis left me with a complete dataframe I could work with, with the options of getting individual athletes data as needed,\n\n::: {.cell .column-page hash='index_cache/html/unnamed-chunk-24_1bb4b93c06f6e2a6a20ca642425e0620'}\n\n```{.r .cell-code}\nclean_ranks <- finaljoin(ranks = ranks, ids = ids, gender = \"M\")\n```\n\n::: {.cell-output-stderr}\n```\nAdding missing grouping variables: `event`\n```\n:::\n\n```{.r .cell-code}\nclean_ranks %>%\n  head() %>%\n  select(rank, perf, name, year, club, venue, athleteurl) %>% \n  kable()\n```\n\n::: {.cell-output-display}\n|rank |perf  |name                   | year|club                 |venue          |athleteurl                                                           |\n|:----|:-----|:----------------------|----:|:--------------------|:--------------|:--------------------------------------------------------------------|\n|1    |10.02 |Dwain Chambers         | 2012|Belgrave             |Olympic Park   |https://www.thepowerof10.info/athletes/profile.aspx?athleteid=31816  |\n|2    |10.05 |Adam Gemili            | 2012|Blackheath & Bromley |Barcelona, ESP |https://www.thepowerof10.info/athletes/profile.aspx?athleteid=208735 |\n|3    |10.13 |James Dasaolu          | 2012|Croydon              |Olympic Park   |https://www.thepowerof10.info/athletes/profile.aspx?athleteid=22721  |\n|4    |10.20 |Harry Aikines-Aryeetey | 2012|Sutton & District    |Rovereto, ITA  |https://www.thepowerof10.info/athletes/profile.aspx?athleteid=19988  |\n|5    |10.21 |Mark Lewis-Francis     | 2012|Birchfield H         |Mesa AZ, USA   |https://www.thepowerof10.info/athletes/profile.aspx?athleteid=21139  |\n|6    |10.22 |James Alaka            | 2012|Blackheath & Bromley |Eugene OR, USA |https://www.thepowerof10.info/athletes/profile.aspx?athleteid=22255  |\n:::\n:::\n\n\nThe final function I used was to scrape and clean each individuals rankings \"on the fly\". \nThe only input is an athletes individual url. \nThe result was another dataframe that contains an individual athletes history of performances.\n\n::: {.cell hash='index_cache/html/unnamed-chunk-26_1069cdc33c24262fdfffc951a7b98a93'}\n\n```{.r .cell-code}\nindividual <- function(athlete) {\n  history <- athlete %>%\n    read_html() %>%\n    html_nodes(xpath = '//*[@id = \"cphBody_pnlPerformances\"]/table') %>%\n    html_table(fill = TRUE) %>%\n    .[[2]] %>%\n    select(-c(X4, X5, X8, X9)) %>%\n    set_names(c(\"event\", \n                \"perf\", \n                \"indoor\", \n                \"position\", \n                \"heat\", \n                \"venue\", \n                \"meeting\", \n                \"date\")) %>%\n    filter(!str_detect(event, \"[:alpha:]\")) %>%\n    mutate(\n      date = dmy(date),\n      year = substr(date, 1, 4),\n      perf_time = case_when(\n        str_detect(perf, \":\") == FALSE ~ str_c(\"00:00:\", perf),\n        str_length(str_split_fixed(perf, \".\", 4)[4]) == 3 ~ str_c(perf, \"0\"),\n        str_length(perf) == 6 | str_length(perf) == 7 ~ str_c(\"00:0\", perf),\n        str_length(perf) == 8 | str_length(perf) == 9 ~ str_c(\"00:\", perf),\n        TRUE ~ perf\n      )\n    )\n  name <- read_html(athlete)\n  name <- html_text(html_nodes(name, css = \"h2\"), trim = TRUE)\n  history <- history %>%\n    mutate(name = name)\n  return(history)\n}\n```\n:::\n\n\n::: {.cell .column-page hash='index_cache/html/unnamed-chunk-28_94861be0c5442b6ba9ecace86c46fd5f'}\n\n```{.r .cell-code}\nathlete <- \"https://www.thepowerof10.info/athletes/profile.aspx?athleteid=31816\"\n\nindividual(athlete) %>%\n  head(5) %>%\n  select(name, event, perf, position, venue, meeting, date) %>% \n  kable()\n```\n\n::: {.cell-output-display}\n|name           |event |perf  |position |venue      |meeting                                             |date       |\n|:--------------|:-----|:-----|:--------|:----------|:---------------------------------------------------|:----------|\n|Dwain Chambers |100   |10.91 |4        |Sportcity  |Müller British Championships inc. Invitation 10000m |2021-06-25 |\n|Dwain Chambers |60    |6.73  |1        |Lee Valley |South of England AA U20 / Senior Championships      |2020-02-01 |\n|Dwain Chambers |60    |6.74  |1        |Lee Valley |South of England AA U20 / Senior Championships      |2020-02-01 |\n|Dwain Chambers |60    |6.76  |2        |Lee Valley |London U20 / Senior Games                           |2020-01-19 |\n|Dwain Chambers |60    |6.77  |5        |Lee Valley |Scienhealth Athletics Invitational                  |2020-01-05 |\n:::\n:::\n\nSo that's it! \nI am currently in the process of creating an interactive dashboard to visualise these results. \nYou can see an early version here: \n\nhttps://harryfish.shinyapps.io/resultsdashboard/\n\nLots of work left to do, but any comments or feedback are always welcome. \nThe source code is on my [github](https://github.com/HarryFisher1/power_of_10) if you want to try anything out.\n\nThanks for reading!",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}